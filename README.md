# DigitalXC Data Analyst Coding Challenge â€“ End-to-End Solution

This repository presents a complete solution to the Data Analyst challenge provided by DigitalXC. It includes end-to-end data engineering and analytics pipeline development using:

- Apache Airflow
- DBT (Data Build Tool)
- PostgreSQL
- Apache Superset

---

## ğŸ“‚ Repository Structure


---

## ğŸ›  Tech Stack Used

- **PostgreSQL** for data storage.
- **DBT** for data transformation and modeling.
- **Apache Airflow** to orchestrate the pipeline.
- **Apache Superset** for building interactive dashboards.

---

## ğŸ§± DBT Models

- `stg_cleaned_tickets.sql`: Cleans and formats the raw data.
- `dim_date_parts.sql`: Extracts Year, Month, Day from the Created Date.
- `fct_avg_resolution.sql`: Calculates average resolution time by Category and Priority.
- `fct_closure_rate.sql`: Computes closure rate by Assigned Group.
- `fct_monthly_summary.sql`: Summarizes monthly metrics.

âœ”ï¸ Detailed SQL code and logic are included in the `1. Data Cleaning & Transformation (DBT).docx`.

---

## ğŸ—‚ Airflow DAG

The DAG named `itsm_pipeline.py` is responsible for:
1. Ingesting CSV into PostgreSQL.
2. Running DBT models.
3. Validating transformation completion.

â± The DAG is scheduled to run **daily** (`@daily`).

ğŸ“„ DAG code and explanation provided in `2. Workflow Orchestration (Apache Airflow).docx`.

---

## ğŸ“Š Superset Dashboard

Built in Apache Superset and exported as `dashboard_export.json`, the dashboard includes:
- **Line Chart** â€“ Daily ticket volume trends.
- **Bar Chart** â€“ Avg. resolution time by category.
- **Pie Chart** â€“ Closure rate by assigned group.
- **Table** â€“ Ticket backlog by priority.
- **Filters** â€“ Week, Category, Priority.

ğŸ“„ See `3. Dashboard & Visualisation (Superset).docx` for layout and explanation.

---

## ğŸš€ How to Run Locally

### 1. PostgreSQL
- Create a DB `itsm`
- Load `ticket_dump.csv` into a table `raw_tickets`

### 2. DBT
```bash
cd dbt_project
dbt run

